> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [www.zhihu.com](https://www.zhihu.com/question/39523290)

> 我在矩阵理论里面没有找到这个。按照wikipedia上的定义，可以把矩阵对矩阵的导数写成一个超矩阵，但是我…

![](https://pic1.zhimg.com/v2-b58b3bb75d9aa70ccff4b5896269d88e_xs.jpg?source=1940ef5c)猪猪专业户. css-1cd9gw4{margin-left:.3em;}

如果题主学过泛函分析，可能会更容易理解矩阵对矩阵的求导。

**定义：**假设![](https://www.zhihu.com/equation?tex=X)和![](https://www.zhihu.com/equation?tex=Y)为赋范向量空间，![](https://www.zhihu.com/equation?tex=F%3A+X%5Crightarrow+Y)是一个映射，那么![](https://www.zhihu.com/equation?tex=F)在![](https://www.zhihu.com/equation?tex=x_0+%5Cin+X)可导的意思是说存在一个有界线性算子![](https://www.zhihu.com/equation?tex=L+%5Cin+%5Cmathcal%7BL%7D%28X%2C+Y%29)，使得对于任意的![](https://www.zhihu.com/equation?tex=%5Cepsilon+%3E+0)都存在![](https://www.zhihu.com/equation?tex=%5Cdelta+%3E+0)，对于满足![](https://www.zhihu.com/equation?tex=x+%5Cin+X+%5Cbackslash+%5C%7Bx_0%5C%7D%2C+%5C%7Cx+-+x_0%5C%7C+%3C+%5Cdelta)的![](https://www.zhihu.com/equation?tex=x)都有![](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5C%7CF%28x%29+-+F%28x_0%29+-+L%28x+-+x_0%29%5C%7C%7D%7B%5C%7Cx+-+x_0%5C%7C%7D+%3C+%5Cepsilon). 我们称![](https://www.zhihu.com/equation?tex=L)为![](https://www.zhihu.com/equation?tex=F)在![](https://www.zhihu.com/equation?tex=x_0)点的导数。

取一些特殊情况，比如当![](https://www.zhihu.com/equation?tex=X+%3D+%5Cmathbb+R%5En%2C+Y+%3D+%5Cmathbb+R)的时候![](https://www.zhihu.com/equation?tex=L)就被称作梯度；当![](https://www.zhihu.com/equation?tex=X+%3D+%5Cmathbb%7BR%7D%5En%2C+Y+%3D+%5Cmathbb%7BR%7D%5Em)的时候![](https://www.zhihu.com/equation?tex=L)被称作雅可比，等等。从这个一般化的定义出发的好处是，我们可以更好的理解矩阵到矩阵映射的 "导数"，甚至是从一个函数空间到另一个函数空间的 “导数 "。

以上定义有一个等价的表述，往往计算起来更方便：对于距离![](https://www.zhihu.com/equation?tex=x_0)足够近的点![](https://www.zhihu.com/equation?tex=x)，即![](https://www.zhihu.com/equation?tex=%5Clim_%7Bx+%5Crightarrow+x_0%7D%5Cfrac%7Bo%28%5C%7Cx-x_0%5C%7C%29%7D%7B%5C%7Cx-x_0%5C%7C%7D+%3D+0)，有  
![](https://www.zhihu.com/equation?tex=F%28x%29+%3D+F%28x_0%29+%2B+L%28x+-+x_0%29+%2B+o%28%5C%7Cx+-+x_0%5C%7C%29.)  
（注：此处![](https://www.zhihu.com/equation?tex=L%28x-x_0%29)应该理解为线性算子![](https://www.zhihu.com/equation?tex=L)在![](https://www.zhihu.com/equation?tex=x+-+x_0)这个点的值，而不是![](https://www.zhihu.com/equation?tex=L)乘以![](https://www.zhihu.com/equation?tex=x-x_0)。不过在有限维空间所有线性算子都可以用矩阵表述，![](https://www.zhihu.com/equation?tex=L)在![](https://www.zhihu.com/equation?tex=x+-+x_0)这个点的值便正好可以表述为矩阵与向量的乘积！这个 notation 正好巧妙的一致。）

**例子：**假设![](https://www.zhihu.com/equation?tex=F%28X%29+%3D+X%5ETX)是一个![](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7Bm%5Ctimes+n%7D+%5Crightarrow+%5Cmathbb%7B+S%7D%5En)的映射，其中![](https://www.zhihu.com/equation?tex=%5Cmathbb%7BS+%7D%5En)为 n 维对称阵的空间。那么![](https://www.zhihu.com/equation?tex=F)的导数![](https://www.zhihu.com/equation?tex=L)就应该是![](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7Bm%5Ctimes+n%7D+%5Crightarrow+%5Cmathbb%7B+S%7D%5En)的一个有界线性算子。![](https://www.zhihu.com/equation?tex=L)究竟是什么样可以从定义出发计算：

![](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%26F%28X%2B%5CDelta+X%29+-+F%28X%29+%5C%5C+%3D%26+%28X%2B%5CDelta+X%29%5ET%28X%2B%5CDelta+X%29+-+X%5ETX%5C%5C+%3D%26+X%5ET%5CDelta+X+%2B+%5CDelta+X%5ETX+%2B+o%28%5C%7C%5CDelta+X%5C%7C%29+%5Cend%7Balign%7D)

所以我们有![](https://www.zhihu.com/equation?tex=L%28%5CDelta+X%29+%3D+X%5ET%5CDelta+X+%2B+%5CDelta+X%5ETX)，这个就是![](https://www.zhihu.com/equation?tex=F)在![](https://www.zhihu.com/equation?tex=X)点的导数。![](https://www.zhihu.com/equation?tex=L)这个函数 (有界线性算子) 可以用张量来表述，这里就不详细说了。

**例子：**最小二乘问题![](https://www.zhihu.com/equation?tex=f%28x%29+%3D+%5Cfrac%7B1%7D%7B2%7D%5C%7CAx-b%5C%7C_2%5E2)，![](https://www.zhihu.com/equation?tex=f)是一个![](https://www.zhihu.com/equation?tex=%5Cmathbb+R%5En+%5Crightarrow+%5Cmathbb+R)的映射。

![](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%26f%28x%2B%5CDelta+x%29+-+f%28x%29+%5C%5C+%3D%26+%5Cfrac%7B1%7D%7B2%7D%5C%7CA%28x%2B%5CDelta+x%29+-+b%5C%7C%5E2+-+%5Cfrac%7B1%7D%7B2%7D%5C%7CAx+-+b%5C%7C%5E2%5C%5C+%3D%26+%5Cfrac%7B1%7D%7B2%7D%5C%7CAx+-+b+%2B+A%5CDelta+x%5C%7C%5E2+-+%5Cfrac%7B1%7D%7B2%7D%5C%7CAx+-+b%5C%7C%5E2%5C%5C+%3D%26+%28Ax+-+b%29%5ETA%5CDelta+x+%2B+o%28%5C%7C%5CDelta+x%5C%7C%29+%5Cend%7Balign%7D)

所以我们有![](https://www.zhihu.com/equation?tex=L%28%5CDelta+x%29+%3D+%28Ax+-+b%29%5ETA%5CDelta+x)，这个就是![](https://www.zhihu.com/equation?tex=f)在![](https://www.zhihu.com/equation?tex=x)点的导数。在这种情况下，![](https://www.zhihu.com/equation?tex=L)这个有界线性算子可以用梯度来表述 (recall Riesz 表示定理)：  
![](https://www.zhihu.com/equation?tex=L%28%5CDelta+x%29+%3D+%5Clangle+%5Cnabla+f%28x%29%2C+%5CDelta+x+%5Crangle+%3D+%5Clangle+A%5ET%28Ax+-+b%29%2C+%5CDelta+x+%5Crangle+%3D+%28Ax-b%29%5ETA%5CDelta+x)  
所以梯度![](https://www.zhihu.com/equation?tex=%5Cnabla+f%28x%29+%3D+A%5ET%28Ax+-+b%29)。

**例子：**单层神经网络![](https://www.zhihu.com/equation?tex=f%28W%29+%3D+%5Cfrac%7B1%7D%7B2%7D%5C%7C%5Csigma%28Wx%29+-+y%5C%7C%5E2_2)，![](https://www.zhihu.com/equation?tex=f)是一个![](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7Bm+%5Ctimes+n+%7D+%5Crightarrow+%5Cmathbb%7B+R+%7D)的映射。这里![](https://www.zhihu.com/equation?tex=%5Csigma%3A+%5Cmathbb%7B+R%7D%5Em+%5Crightarrow+%5Cmathbb%7B+R%7D%5Em)是一个 elementwise 的 logistic function。算起来

![](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%26+f%28W%2B%5CDelta+W%29+-+f%28W%29+%5C%5C+%3D%26+%5Cfrac%7B1%7D%7B2%7D%5C%7C%5Csigma%28Wx+%2B+%5CDelta+Wx%29+-+y%5C%7C_2%5E2+-+%5Cfrac%7B1%7D%7B2%7D%5C%7C%5Csigma%28Wx%29+-+y%5C%7C_2%5E2%5C%5C+%3D%26+%5Cfrac%7B1%7D%7B2%7D%5C%7C%5Csigma%28Wx%29+%2B+%5Csigma%28Wx%29%5Codot+%28%5Cmathbf+1_m+-+%5Csigma%28Wx%29%29+%5Codot+%5CDelta+W+x+%2B+o%28%5C%7C%5CDelta+W%5C%7C%29+-+y%5C%7C_2%5E2+-+%5Cfrac%7B1%7D%7B2%7D%5C%7C%5Csigma%28Wx%29+-+y%5C%7C_2%5E2%5C%5C+%3D%26+%28%5Csigma%28Wx%29+-+y%29%5ET%28%5Csigma%28Wx%29%5Codot+%28%5Cmathbf+1_m+-+%5Csigma%28Wx%29%29+%5Codot+%5CDelta+W+x%29+%2B+o%28%5C%7C%5CDelta+W%5C%7C%29+%5Cend%7Balign%7D)

其中![](https://www.zhihu.com/equation?tex=%5Codot)为 Hadamard 乘积（elementwise 乘积），![](https://www.zhihu.com/equation?tex=%5Cmathbf+1_m)为长度为 m 的元素均为 1 的向量。这里我使用了一维 logistic 函数的导数公式。所以  
![](https://www.zhihu.com/equation?tex=L%28%5CDelta+W%29+%3D+%28%5Csigma%28Wx%29+-+y%29%5ET%28%5Csigma%28Wx%29%5Codot+%28%5Cmathbf+1_m+-+%5Csigma%28Wx%29%29+%5Codot+%5CDelta+W+x%29)。  
注：这个例子的倒数第二步到最后一步的计算影射了微积分中的一个重要的思想——链式法则 (chain rule)。链式法则能够成立的本质是![](https://www.zhihu.com/equation?tex=ao%28%5C%7Cx%5C%7C%29%2Bbo%28%5C%7Cx%5C%7C%29+%3D+o%28%5C%7Cx%5C%7C%29)和![](https://www.zhihu.com/equation?tex=o%28%5C%7Cx%5C%7C%29o%28%5C%7Cx%5C%7C%29+%3D+o%28%5C%7Cx%5C%7C%5E2%29)。

最后，由于![](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7Bm+%5Ctimes+n%7D)和![](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7Bmn%7D)是同构的，所以可以通过 vectorization 把矩阵映射到![](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7Bmn%7D)中再进行计算，见 [@SS Wang](https://www.zhihu.com/people/8c3a8eefd51943127bbed4a0eb92e2c4) 的答案。

![](https://pic3.zhimg.com/v2-3b1998dd4c6ee3c46b289eaca4b0f417_xs.jpg?source=1940ef5c)陈狄. css-1cd9gw4{margin-left:.3em;}

我天，现有的答案复杂到了让我怀疑自己到底懂不懂的程度。。。  
来举个栗子吧：![](https://www.zhihu.com/equation?tex=AB+%3D+C)![](https://www.zhihu.com/equation?tex=%5Cleft%5B+%5Cbegin%7Barray%7D%7Bcc%7D+a_1+%26+a_2+%5C%5C+a_3+%26+a_4+%5Cend%7Barray%7D+%5Cright%5D+%5Cleft%5B+%5Cbegin%7Barray%7D%7Bcc%7D+b_1+%26+b_2+%5C%5C+b_3+%26+b_4+%5Cend%7Barray%7D+%5Cright%5D+%3D+%5Cleft%5B+%5Cbegin%7Barray%7D%7Bcc%7D+c_1+%26+c_2+%5C%5C+c_3+%26+c_4+%5Cend%7Barray%7D+%5Cright%5D)  
其中  
![](https://www.zhihu.com/equation?tex=%5Cleft%5C%7B+%5Cbegin%7Barray%7D%7Bc%7D+c_1+%3D+a_1b_1+%2Ba_2b_3+%5C%5C+c_2+%3D+a_1b_2+%2B+a_2b_4+%5C%5C+c_3+%3D+a_3b_1+%2B+a_4b_3+%5C%5C+c_4+%3D+a_3b_2+%2B+a_4b_4+%5Cend%7Barray%7D+%5Cright.)  
那么

![](https://www.zhihu.com/equation?tex=%5Cdfrac%7B%5Cpartial+C%7D%7B%5Cpartial+A%7D+%5CRightarrow+%5Cleft%5C%7B+%5Cbegin%7Barray%7D%7Bc%7D+%5Cpartial+c_1+%2F+%5Cpartial+a_1+%3D+b_1+%5C%5C+%5Cpartial+c_1+%2F+%5Cpartial+a_2+%3D+b_3+%5C%5C+%5Cpartial+c_1+%2F+%5Cpartial+a_3+%3D+0+%5C%5C+%5Cpartial+c_1+%2F+%5Cpartial+a_4+%3D+0+%5C%5C+%5Cpartial+c_2+%2F+%5Cpartial+a_1+%3D+b_2%5C%5C+%5Cvdots+%5C%5C+%5Cpartial+c_4+%2F+%5Cpartial+a_4+%3D+b_4+%5Cend%7Barray%7D+%5Cright.)

即![](https://www.zhihu.com/equation?tex=C)中每一个元素，对于![](https://www.zhihu.com/equation?tex=A)中每一个元素进行求导。转化成标量的形式就好理解了吧~

至于把以上 16 个标量求导写成![](https://www.zhihu.com/equation?tex=4%5Ctimes+4)的矩阵也好还是 16 维的向量也好，大多是为了形式（理论）上的美观，或是方便对求导结果的后续使用，亦或是方便编程实现，**按需自取**，其本质不变。